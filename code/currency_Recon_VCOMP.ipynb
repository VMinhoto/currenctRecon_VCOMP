{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #### Reading image category  100Euro  ##### \n",
      "Reading file  dataset/train/100Euro/100eu_back.jpg\n",
      "Reading file  dataset/train/100Euro/100eu_back_l.jpg\n",
      "Reading file  dataset/train/100Euro/100eu_back_m.jpg\n",
      "Reading file  dataset/train/100Euro/100eu_back_vl.jpg\n",
      "Reading file  dataset/train/100Euro/100eu_front.jpg\n",
      "Reading file  dataset/train/100Euro/100eu_front_l.jpg\n",
      "Reading file  dataset/train/100Euro/100eu_front_m.jpg\n",
      "Reading file  dataset/train/100Euro/100eu_front_vl.jpg\n",
      " #### Reading image category  10Euro  ##### \n",
      "Reading file  dataset/train/10Euro/10eu_back.jpg\n",
      "Reading file  dataset/train/10Euro/10eu_back_l.jpg\n",
      "Reading file  dataset/train/10Euro/10eu_back_m.jpg\n",
      "Reading file  dataset/train/10Euro/10eu_back_vl.jpg\n",
      "Reading file  dataset/train/10Euro/10eu_front.jpg\n",
      "Reading file  dataset/train/10Euro/10eu_front_l.jpg\n",
      "Reading file  dataset/train/10Euro/10eu_front_m.jpg\n",
      "Reading file  dataset/train/10Euro/10eu_front_vl.jpg\n",
      " #### Reading image category  200Euro  ##### \n",
      "Reading file  dataset/train/200Euro/200eu_back.jpg\n",
      "Reading file  dataset/train/200Euro/200eu_back_l.jpg\n",
      "Reading file  dataset/train/200Euro/200eu_back_m.jpg\n",
      "Reading file  dataset/train/200Euro/200eu_back_vl.jpg\n",
      "Reading file  dataset/train/200Euro/200eu_front.jpg\n",
      "Reading file  dataset/train/200Euro/200eu_front_l.jpg\n",
      "Reading file  dataset/train/200Euro/200eu_front_m.jpg\n",
      "Reading file  dataset/train/200Euro/200eu_front_vl.jpg\n",
      " #### Reading image category  20Euro  ##### \n",
      "Reading file  dataset/train/20Euro/20eu_back.jpg\n",
      "Reading file  dataset/train/20Euro/20eu_back_l.jpg\n",
      "Reading file  dataset/train/20Euro/20eu_back_m.jpg\n",
      "Reading file  dataset/train/20Euro/20eu_back_vl.jpg\n",
      "Reading file  dataset/train/20Euro/20eu_front.jpg\n",
      "Reading file  dataset/train/20Euro/20eu_front_l.jpg\n",
      "Reading file  dataset/train/20Euro/20eu_front_m.jpg\n",
      "Reading file  dataset/train/20Euro/20eu_front_vl.jpg\n",
      " #### Reading image category  500Euro  ##### \n",
      "Reading file  dataset/train/500Euro/500eu_back.jpg\n",
      "Reading file  dataset/train/500Euro/500eu_back_l.jpg\n",
      "Reading file  dataset/train/500Euro/500eu_back_m.jpg\n",
      "Reading file  dataset/train/500Euro/500eu_back_vl.jpg\n",
      "Reading file  dataset/train/500Euro/500eu_front.jpg\n",
      "Reading file  dataset/train/500Euro/500eu_front_l.jpg\n",
      "Reading file  dataset/train/500Euro/500eu_front_m.jpg\n",
      "Reading file  dataset/train/500Euro/500eu_front_vl.jpg\n",
      " #### Reading image category  50Euro  ##### \n",
      "Reading file  dataset/train/50Euro/50eu_back.jpg\n",
      "Reading file  dataset/train/50Euro/50eu_back_l.jpg\n",
      "Reading file  dataset/train/50Euro/50eu_back_m.jpg\n",
      "Reading file  dataset/train/50Euro/50eu_back_vl.jpg\n",
      "Reading file  dataset/train/50Euro/50eu_front.jpg\n",
      "Reading file  dataset/train/50Euro/50eu_front_l.jpg\n",
      "Reading file  dataset/train/50Euro/50eu_front_m.jpg\n",
      "Reading file  dataset/train/50Euro/50eu_front_vl.jpg\n",
      " #### Reading image category  5Euro  ##### \n",
      "Reading file  dataset/train/5Euro/5eu_back.jpg\n",
      "Reading file  dataset/train/5Euro/5eu_back_2002.jpg\n",
      "Reading file  dataset/train/5Euro/5eu_back_2002_l.jpg\n",
      "Reading file  dataset/train/5Euro/5eu_back_2002_m.jpg\n",
      "Reading file  dataset/train/5Euro/5eu_back_2002_vl.jpg\n",
      "Reading file  dataset/train/5Euro/5eu_back_l.jpg\n",
      "Reading file  dataset/train/5Euro/5eu_back_m.jpg\n",
      "Reading file  dataset/train/5Euro/5eu_back_vl.jpg\n",
      "Reading file  dataset/train/5Euro/5eu_front.jpg\n",
      "Reading file  dataset/train/5Euro/5eu_front_2002.jpg\n",
      "Reading file  dataset/train/5Euro/5eu_front_2002_l.jpg\n",
      "Reading file  dataset/train/5Euro/5eu_front_2002_m.jpg\n",
      "Reading file  dataset/train/5Euro/5eu_front_2002_vl.jpg\n",
      "Reading file  dataset/train/5Euro/5eu_front_l.jpg\n",
      "Reading file  dataset/train/5Euro/5eu_front_m.jpg\n",
      "Reading file  dataset/train/5Euro/5eu_front_vl.jpg\n"
     ]
    }
   ],
   "source": [
    "#currency_Recon_VCOMP Pipeline\n",
    "#Libray Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "#data_folder = Path(\"source_data/text_files/\")\n",
    "\n",
    "#file_to_open = data_folder / \"raw_data.txt\"\n",
    "\n",
    "#f = open(file_to_open)\n",
    "\n",
    "#print(f.read())\n",
    "\n",
    "#Read Images\n",
    "#Read and prepare file lists\n",
    "#Read files\n",
    "imlist = {}\n",
    "count = 0\n",
    "#Windows Users:\n",
    "#path = Path(r\"C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\")\n",
    "#path = r\"C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\"\n",
    "#imlist = {}\n",
    "#count = 0\n",
    "#for each in glob(path + \"\\\\*\"):\n",
    "    #word = each.split(\"\\\\\")[-1]\n",
    "    #print(\" #### Reading image category \", word, \" ##### \")\n",
    "    #imlist[word] = []\n",
    "    #for imagefile in glob(path+\"\\\\\"+word+\"\\\\*\"):\n",
    "        #print(\"Reading file \", imagefile)\n",
    "        #im = cv2.imread(imagefile)\n",
    "        #imlist[word].append(im)\n",
    "        #count +=1 \n",
    "\n",
    "#Unix Users\n",
    "imlist = {}\n",
    "count = 0\n",
    "path = \"dataset/train/\"\n",
    "for each in glob(path + \"*\"):\n",
    "    word = each.split(\"/\")[-1]\n",
    "    print(\" #### Reading image category \", word, \" ##### \")\n",
    "    imlist[word] = []\n",
    "    for imagefile in glob(path+word+\"/*\"):\n",
    "        print(\"Reading file \", imagefile)\n",
    "        im = cv2.imread(imagefile)\n",
    "        imlist[word].append(im)\n",
    "        count +=1 \n",
    "#self.images, self.trainImageCount = self.file_helper.getFiles(self.train_path)\n",
    "#for dirName, subdirList, fileList in os.walk(path):\n",
    "    #print('Found directory: %s' % dirName.split(\"/\"))\n",
    "    #for fname in fileList:\n",
    "        #print('\\t%s' % fname)\n",
    "#dirName\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process  and feature computing functions\n",
    "def preprocess(image):\n",
    "    blur = cv2.bilateralFilter(image,8,16,12)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cll = clahe.apply(blur)\n",
    "    return cll\n",
    "\n",
    "\n",
    "def gray(image):\n",
    "    g_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return g_img\n",
    "\n",
    "def features(image):\n",
    "    feat = cv2.xfeatures2d.SIFT_create() \n",
    "    keypoints, descriptors = feat.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Features for  100Euro\n",
      "Computing Features for  10Euro\n",
      "Computing Features for  200Euro\n",
      "Computing Features for  20Euro\n",
      "Computing Features for  500Euro\n",
      "Computing Features for  50Euro\n",
      "Computing Features for  5Euro\n"
     ]
    }
   ],
   "source": [
    "#Pre-process images and extract features\n",
    "label_count = 0\n",
    "name_dict = {}\n",
    "train_labels = np.array([])\n",
    "descriptor_list = []\n",
    "for word in imlist.keys():\n",
    "    name_dict[str(label_count)] = word\n",
    "    print (\"Computing Features for \", word)\n",
    "    for im in imlist[word]:\n",
    "        #cv2.imshow(\"im\", im)\n",
    "        # cv2.waitKey()\n",
    "        train_labels = np.append(train_labels, label_count)\n",
    "        gray_img = gray(im)\n",
    "        preprocessed_img = preprocess(gray_img)\n",
    "        kp, des = features(preprocessed_img)\n",
    "        descriptor_list.append(des)\n",
    "    label_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Histogram Generated\n"
     ]
    }
   ],
   "source": [
    "#Perform clustering\n",
    "#Let's create first the feature array, with the descriptors\n",
    "vStack = np.array(descriptor_list[0])\n",
    "for remaining in descriptor_list[1:]:\n",
    "    vStack = np.vstack((vStack, remaining))\n",
    "    descriptor_vstack = vStack.copy()\n",
    "    #return vStack\n",
    "\n",
    "#Initializa KMeans Classifier\n",
    "#Number of Clusters for KMeans Classifier\n",
    "n_clusters = 100\n",
    "clf = KMeans(n_clusters = n_clusters)\n",
    "kmean_predictions = clf.fit_predict(descriptor_vstack)\n",
    "\n",
    "#Create a Vocabulary of Visual Words\n",
    "n_images = count\n",
    "mega_histogram = np.array([np.zeros(n_clusters) for i in range(n_images)])\n",
    "old_count = 0\n",
    "for i in range(n_images):\n",
    "    l = len(descriptor_list[i])\n",
    "    for j in range(l):\n",
    "        if kmean_predictions is None:\n",
    "            idx = kmean_predictions[old_count+j]\n",
    "        else:\n",
    "            idx = kmean_predictions[old_count+j]\n",
    "        mega_histogram[i][idx] += 1\n",
    "    old_count += l\n",
    "print(\"Vocabulary Histogram Generated\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#self.bov_helper.developVocabulary(n_images = self.trainImageCount, descriptor_list=self.descriptor_list)\n",
    "\n",
    "# show vocabulary trained\n",
    "\n",
    "# self.bov_helper.plotHist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( mega_histogram, open( \"mega_histogram_100.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "Train labels [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 4. 4. 4. 4. 4. 4. 4. 4. 5. 5. 5. 5. 5. 5. 5. 5.\n",
      " 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6.]\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "#Rescale histogram and normalize features\n",
    "#self.bov_helper.standardize()\n",
    "scale = StandardScaler().fit(mega_histogram)\n",
    "mega_histogram = scale.transform(mega_histogram)\n",
    "\n",
    "#Train a classifier: in this case we are using a Support Vector Machine\n",
    "#Initialize SVM Classifier\n",
    "clf  = SVC()\n",
    "print(\"Training SVM\")\n",
    "print(clf)\n",
    "print(\"Train labels\", train_labels)\n",
    "clf.fit(mega_histogram, train_labels)\n",
    "print(\"Training completed\")\n",
    "#self.bov_helper.train(self.train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 100)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(mega_histogram)\n",
    "mega_histogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
       "       max_iter=100000, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
       "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier(max_iter=100000, tol=1e-3)\n",
    "clf.fit(mega_histogram, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 6., 6., 6., 0., 6., 6., 6., 1., 6., 6., 6., 1., 6., 6., 6., 6.,\n",
       "       6., 6., 6., 6., 6., 6., 6., 0., 6., 6., 6., 3., 6., 6., 6., 4., 6.,\n",
       "       6., 6., 4., 6., 6., 6., 5., 6., 6., 6., 5., 6., 6., 6., 6., 0., 6.,\n",
       "       6., 6., 6., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(mega_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
       "      fit_intercept=True, max_iter=None, n_iter=None, n_iter_no_change=5,\n",
       "      n_jobs=None, penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
       "      validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "clf = Perceptron(tol=1e-3, random_state=0)\n",
    "clf.fit(mega_histogram, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 6., 6., 6., 0., 6., 6., 6., 1., 6., 6., 6., 1., 6., 6., 6., 6.,\n",
       "       6., 6., 6., 2., 6., 6., 6., 3., 6., 6., 6., 3., 6., 6., 6., 4., 6.,\n",
       "       6., 6., 4., 6., 6., 6., 5., 6., 6., 6., 5., 6., 6., 6., 6., 6., 6.,\n",
       "       6., 6., 6., 6., 6., 6., 1., 6., 6., 6., 6., 6., 6.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(mega_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
