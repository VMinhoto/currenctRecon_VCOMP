{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-11-8924886a571e>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-8924886a571e>\"\u001b[1;36m, line \u001b[1;32m17\u001b[0m\n\u001b[1;33m    path = (r\"C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\dataset\\\")\u001b[0m\n\u001b[1;37m                                                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "#currency_Recon_VCOMP Pipeline\n",
    "#Libray Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Read Images\n",
    "#Read and prepare file lists\n",
    "#Read files\n",
    "imlist = {}\n",
    "count = 0\n",
    "path =\n",
    "for each in glob(path + \"*\"):\n",
    "    word = each.split(\"/\")[-1]\n",
    "    print (\" #### Reading image category \", word, \" ##### \")\n",
    "    imlist[word] = []\n",
    "    for imagefile in glob(path+word+\"/*\"):\n",
    "        print (\"Reading file \", imagefile)\n",
    "        im = cv2.imread(imagefile, 0)\n",
    "        imlist[word].append(im)\n",
    "        count +=1\n",
    "\n",
    "\n",
    "#self.images, self.trainImageCount = self.file_helper.getFiles(self.train_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process  and feature computing functions\n",
    "def preprocess(image):\n",
    "    blur = cv2.bilateralFilter(img,8,16,12)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cll = clahe.apply(blur)\n",
    "    return cll\n",
    "\n",
    "\n",
    "def gray(self, image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return gray\n",
    "\n",
    "def features(self, image):\n",
    "    feat = cv2.xfeatures2d.SIFT_create() \n",
    "    keypoints, descriptors = feat.sift_object.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C:\\\\Users\\\\tiago\\\\Documents\\\\GitHub\\\\currenctRecon_VCOMP\\\\dataset': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pre-process images and extract features\n",
    "label_count = 0 \n",
    "for word, imlist in self.images.iteritems():\n",
    "    self.name_dict[str(label_count)] = word\n",
    "    print (\"Computing Features for \", word)\n",
    "    for im in imlist:\n",
    "        #cv2.imshow(\"im\", im)\n",
    "        # cv2.waitKey()\n",
    "        train_labels = np.append(train_labels, label_count)\n",
    "        gray_img = gray(im)\n",
    "        preprocessed_img = preprocess(gray)\n",
    "        kp, des = features(preprocessed_img)\n",
    "        descriptor_list.append(des)\n",
    "        label_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform clustering\n",
    "#Let's create first the feature array, with the descriptors\n",
    "vStack = np.array(descriptor_list[0])\n",
    "for remaining in l[1:]:\n",
    "    vStack = np.vstack((vStack, remaining))\n",
    "    descriptor_vstack = vStack.copy()\n",
    "    #return vStack\n",
    "\n",
    "#Initializa KMeans Classifier\n",
    "#Number of Clusters for KMeans Classifier\n",
    "n_clusters = 20\n",
    "clf = KMeans(n_clusters = n_clusters)\n",
    "kmean_predictions = clf.fit_predict(descriptor_vstack)\n",
    "\n",
    "#Create a Vocabulary of Visual Words\n",
    "mega_histogram = np.array([np.zeros(n_clusters) for i in range(n_images)])\n",
    "old_count = 0\n",
    "for i in range(n_images):\n",
    "    l = len(descriptor_list[i])\n",
    "    for j in range(l):\n",
    "        if kmean_predictions is None:\n",
    "            idx = self.kmeans_ret[old_count+j]\n",
    "        else:\n",
    "            idx = kmeans_ret[old_count+j]\n",
    "        mega_histogram[i][idx] += 1\n",
    "    old_count += l\n",
    "print(\"Vocabulary Histogram Generated\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "self.bov_helper.developVocabulary(n_images = self.trainImageCount, descriptor_list=self.descriptor_list)\n",
    "\n",
    "# show vocabulary trained\n",
    "# self.bov_helper.plotHist()\n",
    "self.bov_helper.standardize()\n",
    "self.bov_helper.train(self.train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
