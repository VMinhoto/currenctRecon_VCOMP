{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLANN Algorithm Test\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os, os.path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load Test Images\n",
    "imageDir = \"test/\" #specify your path here\n",
    "image_path_list = []\n",
    "test_images = []\n",
    "test_labels = []\n",
    "valid_image_extensions = [\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"] #specify your vald extensions here\n",
    "valid_image_extensions = [item.lower() for item in valid_image_extensions]\n",
    "\n",
    "for file in os.listdir(imageDir):\n",
    "    extension = os.path.splitext(file)[1]\n",
    "    if extension.lower() not in valid_image_extensions:\n",
    "        continue\n",
    "    image_path_list.append(os.path.join(imageDir, file))\n",
    "\n",
    "for imagePath in image_path_list:\n",
    "    test_labels.append(imagePath)\n",
    "    test_images.append(cv2.cvtColor(cv2.imread(imagePath), cv2.COLOR_BGR2GRAY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Keypoints and Descriptors From Train Images\n",
    "imageDir = \"train/\" #specify your path here\n",
    "image_path_list = []\n",
    "train_images = []\n",
    "train_labels = []\n",
    "valid_image_extensions = [\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"] #specify your vald extensions here\n",
    "valid_image_extensions = [item.lower() for item in valid_image_extensions]\n",
    "\n",
    "for file in os.listdir(imageDir):\n",
    "    extension = os.path.splitext(file)[1]\n",
    "    if extension.lower() not in valid_image_extensions:\n",
    "        continue\n",
    "    image_path_list.append(os.path.join(imageDir, file))\n",
    "\n",
    "for imagePath in image_path_list:\n",
    "    train_labels.append(imagePath)\n",
    "    train_images.append(cv2.cvtColor(cv2.imread(imagePath), cv2.COLOR_BGR2GRAY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/100eu_back.jpg\n",
      "train/100eu_front.jpg\n",
      "train/10eu_back.jpg\n",
      "train/10eu_front.jpg\n",
      "train/200eu_back.jpg\n",
      "train/200eu_front.jpg\n",
      "train/20eu_back.jpg\n",
      "train/20eu_front.jpg\n",
      "train/500eu_back.jpg\n",
      "train/500eu_front.jpg\n",
      "train/50eu_back.jpg\n",
      "train/50eu_front.jpg\n",
      "train/5eu_back.jpg\n",
      "train/5eu_back_2002.jpg\n",
      "train/5eu_front.jpg\n",
      "train/5eu_front_2002.jpg\n"
     ]
    }
   ],
   "source": [
    "for label in train_labels:\n",
    "    print(label)\n",
    "    \n",
    "train_labels_text = [\"100EurB\", \"100EurF\",\"10EurB\", \"10EurF\", \"200EurB\", \"200EurF\", \"20EurB\", \"20EurF\", \"500EurB\", \"500EurF\",\n",
    "                \"50EurB\", \"50EurF\", \"5EurB\", \"5Eur2002B\", \"5EurF\", \"5Eur2002F\"]\n",
    "\n",
    "import pickle\n",
    "with open('train_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(train_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Keypoints and Descriptors For Each Train Image\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "#surf = cv2.xfeatures2d.SURF_create(400)\n",
    "descriptors = []\n",
    "\n",
    "for image in train_images:\n",
    "    kp, des = sift.detectAndCompute(image, None)\n",
    "    descriptors.append(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Descriptors List so we can use them after\n",
    "import pickle\n",
    "with open('descriptors_surf.pkl', 'wb') as f:\n",
    "    pickle.dump(descriptors, f)\n",
    "\n",
    "#Load Descriptors List\n",
    "#import pickle\n",
    "#descriptors = pickle.load(open('descriptors.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21059"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's test clustering for bag of visual words\n",
    "vStack = np.array(descriptors[0])\n",
    "for remaining in descriptors[1:]:\n",
    "    vStack = np.vstack((vStack, remaining))\n",
    "    descriptor_vstack = vStack.copy()\n",
    "\n",
    "#from sklearn.cluster import KMeans\n",
    "#X = np.array(descriptor_vstack)\n",
    "#kmeans = KMeans(n_clusters=9, random_state=0)\n",
    "#kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's get each image as an image clustered words\n",
    "image_clustered_words = []\n",
    "for descriptor in descriptors:\n",
    "    image_clustered_words.append(kmeans.predict(descriptor))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally make a histogram of clustered word counts for each image. These are the final features.\n",
    "img_bow_hist = np.array(\n",
    "[np.bincount(clustered_words, minlength=9) for clustered_words in image_clustered_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 14 12 15 13]\n"
     ]
    }
   ],
   "source": [
    "#Encode labels\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_labels_text)\n",
    "le.classes_\n",
    "train_labels_encoded = le.transform(train_labels_text)\n",
    "#le.inverse_transform(train_labels_encoded)\n",
    "\n",
    "print(train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(537750,) (537750, 64)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "extended_train_labels = []\n",
    "i = 0\n",
    "while i < len(descriptors):\n",
    "    while (count < len(descriptors[i])):\n",
    "        extended_train_labels.append(train_labels_encoded[i])\n",
    "        count = count + 1\n",
    "    i = i + 1\n",
    "    count = 0\n",
    "print(np.array(extended_train_labels).shape, descriptor_vstack.shape)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17462017666201768"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's Use An SVM To Train Model\n",
    "from sklearn.svm import LinearSVC\n",
    "SVM = LinearSVC(random_state=0, tol=1e-5, max_iter=1000)\n",
    "SVM.fit(descriptor_vstack, extended_train_labels)\n",
    "\n",
    "SVM.score(X=descriptor_vstack, y=extended_train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is:  50EurF\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  50EurF\n",
      "Prediction is:  5Eur2002F\n"
     ]
    }
   ],
   "source": [
    "#Let's test on a new image\n",
    "from collections import Counter\n",
    "for img in test_images:\n",
    "    #sift = cv2.xfeatures2d.SIFT_create()\n",
    "    surf = cv2.xfeatures2d.SURF_create(400)\n",
    "    kp, desc = surf.detectAndCompute(img, None)\n",
    "\n",
    "    #clustered_desc = kmeans.predict(desc)\n",
    "    #img_bow_hist_img = np.bincount(clustered_desc, minlength=9)\n",
    "\n",
    "    # reshape to an array containing 1 array: array[[1,2,3]]\n",
    "    # to make sklearn happy (it doesn't like 1d arrays as data!)\n",
    "    #test_img = desc.reshape(1,-1)\n",
    "    prediction = SVM.predict(desc)\n",
    "    print(\"Prediction is: \", max(Counter(le.inverse_transform(prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img1 = cv2.imread('test/100-100.jpg',0)          # queryImage\n",
    "#img2 = cv2.imread('500eu_front.jpg',0) # trainImage\n",
    "\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "#kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "matches = []\n",
    "\n",
    "for i in range(len(descriptors)):\n",
    "    # FLANN parameters\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks=100)   # or pass empty dictionary\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "    matches.append(flann.knnMatch(des1,descriptors[i],k=2))\n",
    "\n",
    "# Need to draw only good matches, so create a mask\n",
    "#matchesMask = [[0,0] for i in range(len(matches))]\n",
    "\n",
    "# ratio test as per Lowe's paper\n",
    "#for i,(m,n) in enumerate(matches):\n",
    "    #if m.distance < 0.7*n.distance:\n",
    "        #matchesMask[i]=[1,0]\n",
    "\n",
    "#draw_params = dict(matchColor = (0,255,0),\n",
    "                   #singlePointColor = (255,0,0),\n",
    "                   #matchesMask = matchesMask,\n",
    "                   #flags = 0)\n",
    "\n",
    "#img3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,matches,None,**draw_params)\n",
    "\n",
    "#plt.imshow(img3,),plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenghts = []\n",
    "for match in matches:\n",
    "    lenghts.append(len(match))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/5eu_front_2002.jpg\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lenghts)):\n",
    "    if lenghts[i] == max(lenghts):\n",
    "        prediction = train_labels[i]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "#index_params= dict(algorithm = FLANN_INDEX_LSH,\n",
    "                   #table_number = 6, # 12\n",
    "                   #key_size = 12,     # 20\n",
    "                   #multi_probe_level = 1) #2\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img1 = cv2.imread('5.JPG',0)          # queryImage\n",
    "img2 = cv2.imread('500eu_front.jpg',0) # trainImage\n",
    "\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=100)   # or pass empty dictionary\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "matches_5eur = flann.knnMatch(des1,des2,k=5)\n",
    "\n",
    "# Need to draw only good matches, so create a mask\n",
    "matchesMask = [[0,0] for i in range(len(matches_5eur))]\n",
    "\n",
    "# ratio test as per Lowe's paper\n",
    "#for i,(m,n) in enumerate(matches_5eur):\n",
    "    #if m.distance < 0.7*n.distance:\n",
    "        #matchesMask[i]=[1,0]\n",
    "\n",
    "#draw_params = dict(matchColor = (0,255,0),\n",
    "                   #singlePointColor = (255,0,0),\n",
    "                   #matchesMask = matchesMask,\n",
    "                   #flags = 0)\n",
    "\n",
    "#img3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,matches_5eur,None,**draw_params)\n",
    "\n",
    "#plt.imshow(img3,),plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "#index_params= dict(algorithm = FLANN_INDEX_LSH,\n",
    "                   #table_number = 6, # 12\n",
    "                   #key_size = 12,     # 20\n",
    "                   #multi_probe_level = 1) #2\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img1 = cv2.imread('20__(4).JPG',0)          # queryImage\n",
    "img2 = cv2.imread('500eu_front.jpg',0) # trainImage\n",
    "\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=100)   # or pass empty dictionary\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "matches_100eur = flann.knnMatch(des1,des2,k=5)\n",
    "\n",
    "# Need to draw only good matches, so create a mask\n",
    "#matchesMask = [[0,0] for i in range(len(matches_100eur))]\n",
    "\n",
    "# ratio test as per Lowe's paper\n",
    "#for i,(m,n) in enumerate(matches_100eur):\n",
    "    #if m.distance < 0.7*n.distance:\n",
    "        #matchesMask[i]=[1,0]\n",
    "\n",
    "#draw_params = dict(matchColor = (0,255,0),\n",
    "                   #singlePointColor = (255,0,0),\n",
    "                   #matchesMask = matchesMask,\n",
    "                   #flags = 0)\n",
    "\n",
    "#img3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,matches_100eur,None,**draw_params)\n",
    "\n",
    "#plt.imshow(img3,),plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531 313 479\n"
     ]
    }
   ],
   "source": [
    "print(len(matches), len(matches_5eur), len(matches_100eur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading...  test/10__(8).jpg\n"
     ]
    }
   ],
   "source": [
    "# create BFMatcher object\n",
    "bf = cv2.BFMatcher_create()\n",
    "#bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "kp, des2 = sift.detectAndCompute(test_images[15], None)\n",
    "print(\"Reading... \", test_labels[15])\n",
    "\n",
    "# Match descriptors.\n",
    "matches = []\n",
    "for descriptor in descriptors:\n",
    "    matches.append(bf.knnMatch(descriptor,des2, k=2))\n",
    "\n",
    "# Apply ratio test\n",
    "#good = []\n",
    "#for m,n in matches:\n",
    "    #if m.distance < 0.75*n.distance:\n",
    "        #good.append([m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338540 338540\n",
      "Prediction is  500EurB\n"
     ]
    }
   ],
   "source": [
    "mat_len = []\n",
    "for i in range(len(matches)):\n",
    "    mat_len.append(len(matches[i]))\n",
    "\n",
    "for i in range(len(mat_len)):\n",
    "    if mat_len[i] == max(mat_len):\n",
    "        print(mat_len[i], max(mat_len))\n",
    "        print(\"Prediction is \", train_labels_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194725"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
