{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #### Reading image category  100Euro  ##### \n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\100Euro\\100eu_back.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\100Euro\\100eu_back_l.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\100Euro\\100eu_back_m.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\100Euro\\100eu_back_vl.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\100Euro\\100eu_front.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\100Euro\\100eu_front_l.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\100Euro\\100eu_front_m.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\100Euro\\100eu_front_vl.jpg\n",
      " #### Reading image category  10Euro  ##### \n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\10Euro\\10eu_back.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\10Euro\\10eu_back_l.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\10Euro\\10eu_back_m.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\10Euro\\10eu_back_vl.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\10Euro\\10eu_front.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\10Euro\\10eu_front_l.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\10Euro\\10eu_front_m.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\10Euro\\10eu_front_vl.jpg\n",
      " #### Reading image category  200Euro  ##### \n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\200Euro\\200eu_back.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\200Euro\\200eu_back_l.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\200Euro\\200eu_back_m.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\200Euro\\200eu_back_vl.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\200Euro\\200eu_front.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\200Euro\\200eu_front_l.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\200Euro\\200eu_front_m.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\200Euro\\200eu_front_vl.jpg\n",
      " #### Reading image category  20Euro  ##### \n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\20Euro\\20eu_back.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\20Euro\\20eu_back_l.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\20Euro\\20eu_back_m.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\20Euro\\20eu_back_vl.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\20Euro\\20eu_front.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\20Euro\\20eu_front_l.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\20Euro\\20eu_front_m.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\20Euro\\20eu_front_vl.jpg\n",
      " #### Reading image category  500Euro  ##### \n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\500Euro\\500eu_back.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\500Euro\\500eu_back_l.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\500Euro\\500eu_back_m.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\500Euro\\500eu_back_vl.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\500Euro\\500eu_front.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\500Euro\\500eu_front_l.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\500Euro\\500eu_front_m.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\500Euro\\500eu_front_vl.jpg\n",
      " #### Reading image category  50Euro  ##### \n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\50Euro\\50eu_back.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\50Euro\\50eu_back_l.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\50Euro\\50eu_back_m.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\50Euro\\50eu_back_vl.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\50Euro\\50eu_front.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\50Euro\\50eu_front_l.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\50Euro\\50eu_front_m.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\50Euro\\50eu_front_vl.jpg\n",
      " #### Reading image category  5Euro  ##### \n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\5Euro\\5eu_back.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\5Euro\\5eu_back_2002.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\5Euro\\5eu_back_2002_l.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\5Euro\\5eu_back_2002_m.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\5Euro\\5eu_back_2002_vl.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\5Euro\\5eu_back_l.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\5Euro\\5eu_back_m.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\5Euro\\5eu_back_vl.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\5Euro\\5eu_front.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\5Euro\\5eu_front_2002.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\5Euro\\5eu_front_2002_l.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\5Euro\\5eu_front_2002_m.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\5Euro\\5eu_front_2002_vl.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\5Euro\\5eu_front_l.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\5Euro\\5eu_front_m.jpg\n",
      "Reading file  C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\\5Euro\\5eu_front_vl.jpg\n"
     ]
    }
   ],
   "source": [
    "#currency_Recon_VCOMP Pipeline\n",
    "#Libray Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "#data_folder = Path(\"source_data/text_files/\")\n",
    "\n",
    "#file_to_open = data_folder / \"raw_data.txt\"\n",
    "\n",
    "#f = open(file_to_open)\n",
    "\n",
    "#print(f.read())\n",
    "\n",
    "#Read Images\n",
    "#Read and prepare file lists\n",
    "#Read files\n",
    "imlist = {}\n",
    "count = 0\n",
    "#path = Path(r\"C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\")\n",
    "path = r\"C:\\Users\\tiago\\Documents\\GitHub\\currenctRecon_VCOMP\\code\\dataset\\train\"\n",
    "imlist = {}\n",
    "count = 0\n",
    "for each in glob(path + \"\\\\*\"):\n",
    "    word = each.split(\"\\\\\")[-1]\n",
    "    print(\" #### Reading image category \", word, \" ##### \")\n",
    "    imlist[word] = []\n",
    "    for imagefile in glob(path+\"\\\\\"+word+\"\\\\*\"):\n",
    "        print(\"Reading file \", imagefile)\n",
    "        im = cv2.imread(imagefile)\n",
    "        imlist[word].append(im)\n",
    "        count +=1 \n",
    "\n",
    "\n",
    "#self.images, self.trainImageCount = self.file_helper.getFiles(self.train_path)\n",
    "#for dirName, subdirList, fileList in os.walk(path):\n",
    "    #print('Found directory: %s' % dirName.split(\"/\"))\n",
    "    #for fname in fileList:\n",
    "        #print('\\t%s' % fname)\n",
    "#dirName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process  and feature computing functions\n",
    "def preprocess(image):\n",
    "    blur = cv2.bilateralFilter(image,8,16,12)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cll = clahe.apply(blur)\n",
    "    return cll\n",
    "\n",
    "\n",
    "def gray(image):\n",
    "    g_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return g_img\n",
    "\n",
    "def features(image):\n",
    "    feat = cv2.xfeatures2d.SIFT_create() \n",
    "    keypoints, descriptors = feat.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Features for  100Euro\n",
      "Computing Features for  10Euro\n",
      "Computing Features for  200Euro\n",
      "Computing Features for  20Euro\n",
      "Computing Features for  500Euro\n",
      "Computing Features for  50Euro\n",
      "Computing Features for  5Euro\n"
     ]
    }
   ],
   "source": [
    "#Pre-process images and extract features\n",
    "label_count = 0\n",
    "name_dict = {}\n",
    "train_labels = np.array([])\n",
    "descriptor_list = []\n",
    "for word in imlist.keys():\n",
    "    name_dict[str(label_count)] = word\n",
    "    print (\"Computing Features for \", word)\n",
    "    for im in imlist[word]:\n",
    "        #cv2.imshow(\"im\", im)\n",
    "        # cv2.waitKey()\n",
    "        train_labels = np.append(train_labels, label_count)\n",
    "        gray_img = gray(im)\n",
    "        preprocessed_img = preprocess(gray_img)\n",
    "        kp, des = features(preprocessed_img)\n",
    "        descriptor_list.append(des)\n",
    "    label_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d06d878c26a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mkmean_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescriptor_vstack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#Create a Vocabulary of Visual Words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vcomp\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    992\u001b[0m             \u001b[0mIndex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0mbelongs\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m         \"\"\"\n\u001b[1;32m--> 994\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vcomp\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    966\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m                 return_n_iter=True)\n\u001b[0m\u001b[0;32m    969\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vcomp\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[1;34m(X, n_clusters, sample_weight, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[0;32m    378\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecompute_distances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_squared_norms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_squared_norms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m                 random_state=random_state)\n\u001b[0m\u001b[0;32m    381\u001b[0m             \u001b[1;31m# determine if these results are the best so far\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vcomp\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[1;34m(X, sample_weight, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[0;32m    442\u001b[0m     centers, labels, n_iter = k_means_elkan(X, checked_sample_weight,\n\u001b[0;32m    443\u001b[0m                                             \u001b[0mn_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m                                             max_iter=max_iter, verbose=verbose)\n\u001b[0m\u001b[0;32m    445\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m         \u001b[0minertia\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcenters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\cluster\\_k_means_elkan.pyx\u001b[0m in \u001b[0;36msklearn.cluster._k_means_elkan.k_means_elkan\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\vcomp\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;31m# Pairwise distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m def euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False,\n\u001b[0m\u001b[0;32m    165\u001b[0m                         X_norm_squared=None):\n\u001b[0;32m    166\u001b[0m     \"\"\"\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Perform clustering\n",
    "#Let's create first the feature array, with the descriptors\n",
    "vStack = np.array(descriptor_list[0])\n",
    "for remaining in descriptor_list[1:]:\n",
    "    vStack = np.vstack((vStack, remaining))\n",
    "    descriptor_vstack = vStack.copy()\n",
    "    #return vStack\n",
    "\n",
    "#Initializa KMeans Classifier\n",
    "#Number of Clusters for KMeans Classifier\n",
    "n_clusters = 20\n",
    "clf = KMeans(n_clusters = n_clusters)\n",
    "kmean_predictions = clf.fit_predict(descriptor_vstack)\n",
    "\n",
    "#Create a Vocabulary of Visual Words\n",
    "mega_histogram = np.array([np.zeros(n_clusters) for i in range(n_images)])\n",
    "old_count = 0\n",
    "for i in range(n_images):\n",
    "    l = len(descriptor_list[i])\n",
    "    for j in range(l):\n",
    "        if kmean_predictions is None:\n",
    "            idx = kmean_predictions[old_count+j]\n",
    "        else:\n",
    "            idx = kmean_predictions[old_count+j]\n",
    "        mega_histogram[i][idx] += 1\n",
    "    old_count += l\n",
    "print(\"Vocabulary Histogram Generated\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#self.bov_helper.developVocabulary(n_images = self.trainImageCount, descriptor_list=self.descriptor_list)\n",
    "\n",
    "# show vocabulary trained\n",
    "\n",
    "# self.bov_helper.plotHist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rescale histogram and normalize features\n",
    "#self.bov_helper.standardize()\n",
    "scale = StandardScaler().fit(mega_histogram)\n",
    "mega_histogram = scale.transform(mega_histogram)\n",
    "\n",
    "#Train a classifier: in this case we are using a Support Vector Machine\n",
    "#Initialize SVM Classifier\n",
    "clf  = SVC()\n",
    "print(\"Training SVM\")\n",
    "print(clf)\n",
    "print(\"Train labels\", train_labels)\n",
    "clf.fit(mega_histogram, train_labels)\n",
    "print(\"Training completed\")\n",
    "#self.bov_helper.train(self.train_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
